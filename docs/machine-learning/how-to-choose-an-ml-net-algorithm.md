---
title: ML.NET 알고리즘을 선택하는 방법
description: 기계 학습 모델에 사용할 ML.NET 알고리즘을 선택하는 방법에 대해 살펴봅니다.
ms.topic: overview
ms.date: 06/05/2019
ms.openlocfilehash: 0fed33203c02303e37e47f548e08ec131eeb1c77
ms.sourcegitcommit: 7588136e355e10cbc2582f389c90c127363c02a5
ms.translationtype: HT
ms.contentlocale: ko-KR
ms.lasthandoff: 03/15/2020
ms.locfileid: "75739999"
---
# <a name="how-to-choose-an-mlnet-algorithm"></a><span data-ttu-id="d8192-103">ML.NET 알고리즘을 선택하는 방법</span><span class="sxs-lookup"><span data-stu-id="d8192-103">How to choose an ML.NET algorithm</span></span>

<span data-ttu-id="d8192-104">각 [ML.NET 작업](resources/tasks.md)에는 선택할 수 있는 여러 학습 알고리즘이 있습니다.</span><span class="sxs-lookup"><span data-stu-id="d8192-104">For each [ML.NET task](resources/tasks.md), there are multiple training algorithms to choose from.</span></span> <span data-ttu-id="d8192-105">선택할 알고리즘은 사용자가 해결하려고 하는 문제, 데이터 특징, 사용 가능한 컴퓨팅 및 스토리지 리소스에 따라 좌우됩니다.</span><span class="sxs-lookup"><span data-stu-id="d8192-105">Which one to choose depends on the problem you are trying to solve, the characteristics of your data, and the compute and storage resources you have available.</span></span> <span data-ttu-id="d8192-106">기계 학습 모델을 학습하는 것은 반복적인 프로세스입니다.</span><span class="sxs-lookup"><span data-stu-id="d8192-106">It is important to note that training a machine learning model is an iterative process.</span></span> <span data-ttu-id="d8192-107">가장 적합한 것을 찾기 위해 여러 알고리즘을 시도해봐야 합니다.</span><span class="sxs-lookup"><span data-stu-id="d8192-107">You might need to try multiple algorithms to find the one that works best.</span></span>

<span data-ttu-id="d8192-108">알고리즘은 **기능**에서 작동합니다.</span><span class="sxs-lookup"><span data-stu-id="d8192-108">Algorithms operate on **features**.</span></span> <span data-ttu-id="d8192-109">기능은 입력 데이터에서 계산된 숫자 값으로,</span><span class="sxs-lookup"><span data-stu-id="d8192-109">Features are numerical values computed from your input data.</span></span> <span data-ttu-id="d8192-110">기계 학습 알고리즘에 최적의 입력입니다.</span><span class="sxs-lookup"><span data-stu-id="d8192-110">They are optimal inputs for machine learning algorithms.</span></span> <span data-ttu-id="d8192-111">하나 이상의 [데이터 변형](resources/transforms.md)을 사용하여 원시 입력 데이터를 기능을 변환합니다.</span><span class="sxs-lookup"><span data-stu-id="d8192-111">You transform your raw input data into features using one or more [data transforms](resources/transforms.md).</span></span> <span data-ttu-id="d8192-112">예를 들어, 텍스트 데이터는 단어 수와 단어 조합 수 세트로 변환됩니다.</span><span class="sxs-lookup"><span data-stu-id="d8192-112">For example, text data is transformed into a set of word counts and word combination counts.</span></span> <span data-ttu-id="d8192-113">데이터 변형을 사용하여 기능이 원시 데이터 형식에서 추출된 후에는 **기능화**되었다고 합니다.</span><span class="sxs-lookup"><span data-stu-id="d8192-113">Once the features have been extracted from a raw data type using data transforms, they are referred to as **featurized**.</span></span> <span data-ttu-id="d8192-114">기능화된 텍스트 또는 기능화된 이미지 데이터를 예로 들 수 있습니다.</span><span class="sxs-lookup"><span data-stu-id="d8192-114">For example, featurized text, or featurized image data.</span></span>

## <a name="trainer--algorithm--task"></a><span data-ttu-id="d8192-115">트레이너 = 알고리즘 + 작업</span><span class="sxs-lookup"><span data-stu-id="d8192-115">Trainer = Algorithm + Task</span></span>

<span data-ttu-id="d8192-116">알고리즘은 **모델**을 생성하기 위해 실행하는 수학입니다.</span><span class="sxs-lookup"><span data-stu-id="d8192-116">An algorithm is the math that executes to produce a **model**.</span></span> <span data-ttu-id="d8192-117">다른 알고리즘은 다른 특징의 모델을 생성합니다.</span><span class="sxs-lookup"><span data-stu-id="d8192-117">Different algorithms produce models with different characteristics.</span></span>

<span data-ttu-id="d8192-118">ML.NET을 사용하여 동일한 알고리즘을 다른 작업에 적용할 수 있습니다.</span><span class="sxs-lookup"><span data-stu-id="d8192-118">With ML.NET, the same algorithm can be applied to different tasks.</span></span> <span data-ttu-id="d8192-119">예를 들어 확률적 이중 좌표 상승법(Stochastic Descent Coordinated Ascent)을 이진 분류, 다중 클래스 분류 및 회귀에 사용할 수 있습니다.</span><span class="sxs-lookup"><span data-stu-id="d8192-119">For example, Stochastic Dual Coordinated Ascent can be used for Binary Classification, Multiclass Classification, and Regression.</span></span> <span data-ttu-id="d8192-120">차이점은 작업에 맞추기 위해 알고리즘의 출력이 해석되는 방식에 있습니다.</span><span class="sxs-lookup"><span data-stu-id="d8192-120">The difference is in how the output of the algorithm is interpreted to match the task.</span></span>

<span data-ttu-id="d8192-121">각 알고리즘/작업 조합에 대해 ML.NET은 학습 알고리즘을 실행하고 해석을 수행하는 구성 요소를 제공합니다.</span><span class="sxs-lookup"><span data-stu-id="d8192-121">For each algorithm/task combination, ML.NET provides a component that executes the training algorithm and does the interpretation.</span></span> <span data-ttu-id="d8192-122">이러한 구성 요소를 트레이너라고 합니다.</span><span class="sxs-lookup"><span data-stu-id="d8192-122">These components are called trainers.</span></span> <span data-ttu-id="d8192-123">예를 들어 <xref:Microsoft.ML.Trainers.SdcaRegressionTrainer>는 **회귀** 작업에 적용된 **StochasticDualCoordinatedAscent** 알고리즘을 사용합니다.</span><span class="sxs-lookup"><span data-stu-id="d8192-123">For example, the <xref:Microsoft.ML.Trainers.SdcaRegressionTrainer> uses the **StochasticDualCoordinatedAscent** algorithm applied to the **Regression** task.</span></span>

## <a name="linear-algorithms"></a><span data-ttu-id="d8192-124">선형 알고리즘</span><span class="sxs-lookup"><span data-stu-id="d8192-124">Linear algorithms</span></span>

<span data-ttu-id="d8192-125">선형 알고리즘은 입력 데이터의 선형 조합과 **가중치** 세트로부터 **점수**를 계산하는 모델을 생성합니다.</span><span class="sxs-lookup"><span data-stu-id="d8192-125">Linear algorithms produce a model that calculates **scores** from a linear combination of the input data and a set of **weights**.</span></span> <span data-ttu-id="d8192-126">가중치는 학습 중 추정된 모델의 매개 변수입니다.</span><span class="sxs-lookup"><span data-stu-id="d8192-126">The weights are parameters of the model estimated during training.</span></span>

<span data-ttu-id="d8192-127">선형 알고리즘은 [선형적으로 분리 가능한](https://en.wikipedia.org/wiki/Linear_separability) 기능에 잘 작동합니다.</span><span class="sxs-lookup"><span data-stu-id="d8192-127">Linear algorithms work well for features that are [linearly separable](https://en.wikipedia.org/wiki/Linear_separability).</span></span>

<span data-ttu-id="d8192-128">선형 알고리즘으로 학습하기 전에 기능을 정규화해야 합니다.</span><span class="sxs-lookup"><span data-stu-id="d8192-128">Before training with a linear algorithm, the features should be normalized.</span></span> <span data-ttu-id="d8192-129">이를 통해 하나의 기능이 다른 기능에 비해 결과에 더 많은 영향을 끼치지 않게 해줍니다.</span><span class="sxs-lookup"><span data-stu-id="d8192-129">This prevents one feature having more influence over the result than others.</span></span>

<span data-ttu-id="d8192-130">일반적으로 선형 알고리즘은 확장 가능하고 빠르며 학습하기 쉽고 예측이 편리합니다.</span><span class="sxs-lookup"><span data-stu-id="d8192-130">In general linear algorithms are scalable and fast, cheap to train, cheap to predict.</span></span> <span data-ttu-id="d8192-131">기능의 수와 대략적으로 학습 데이터 세트의 크기로 규모가 조정됩니다.</span><span class="sxs-lookup"><span data-stu-id="d8192-131">They scale by the number of features and approximately by the size of the training data set.</span></span>

<span data-ttu-id="d8192-132">선형 알고리즘은 학습 데이터에 여러 개의 통로를 만듭니다.</span><span class="sxs-lookup"><span data-stu-id="d8192-132">Linear algorithms make multiple passes over the training data.</span></span> <span data-ttu-id="d8192-133">데이터 세트가 메모리에 적합할 경우 트레이너를 추가하기 전에 ML.NET 파이프라인에 [캐시 검사점](xref:Microsoft.ML.LearningPipelineExtensions.AppendCacheCheckpoint*)을 추가하면 학습을 보다 빨리 실행할 수 있습니다.</span><span class="sxs-lookup"><span data-stu-id="d8192-133">If your dataset fits into memory, then adding a [cache checkpoint](xref:Microsoft.ML.LearningPipelineExtensions.AppendCacheCheckpoint*) to your ML.NET pipeline before appending the trainer, will make the training run faster.</span></span>

<span data-ttu-id="d8192-134">**선형 트레이너**</span><span class="sxs-lookup"><span data-stu-id="d8192-134">**Linear Trainers**</span></span>

|<span data-ttu-id="d8192-135">알고리즘</span><span class="sxs-lookup"><span data-stu-id="d8192-135">Algorithm</span></span>|<span data-ttu-id="d8192-136">속성</span><span class="sxs-lookup"><span data-stu-id="d8192-136">Properties</span></span>|<span data-ttu-id="d8192-137">트레이너</span><span class="sxs-lookup"><span data-stu-id="d8192-137">Trainers</span></span>|
|---------|----------|--------|
|<span data-ttu-id="d8192-138">평균 퍼셉트론(Averaged perceptron)</span><span class="sxs-lookup"><span data-stu-id="d8192-138">Averaged perceptron</span></span>|<span data-ttu-id="d8192-139">텍스트 분류에 최적</span><span class="sxs-lookup"><span data-stu-id="d8192-139">Best for text classification</span></span>|<xref:Microsoft.ML.Trainers.AveragedPerceptronTrainer>|
|<span data-ttu-id="d8192-140">확률적 이중 좌표 상승</span><span class="sxs-lookup"><span data-stu-id="d8192-140">Stochastic dual coordinated ascent</span></span>|<span data-ttu-id="d8192-141">좋은 기본 성능에는 튜닝이 필요하지 않음</span><span class="sxs-lookup"><span data-stu-id="d8192-141">Tuning not needed for good default performance</span></span>|<span data-ttu-id="d8192-142"><xref:Microsoft.ML.Trainers.SdcaLogisticRegressionBinaryTrainer> <xref:Microsoft.ML.Trainers.SdcaNonCalibratedBinaryTrainer> <xref:Microsoft.ML.Trainers.SdcaMaximumEntropyMulticlassTrainer> <xref:Microsoft.ML.Trainers.SdcaNonCalibratedMulticlassTrainer> <xref:Microsoft.ML.Trainers.SdcaRegressionTrainer></span><span class="sxs-lookup"><span data-stu-id="d8192-142"><xref:Microsoft.ML.Trainers.SdcaLogisticRegressionBinaryTrainer> <xref:Microsoft.ML.Trainers.SdcaNonCalibratedBinaryTrainer> <xref:Microsoft.ML.Trainers.SdcaMaximumEntropyMulticlassTrainer> <xref:Microsoft.ML.Trainers.SdcaNonCalibratedMulticlassTrainer> <xref:Microsoft.ML.Trainers.SdcaRegressionTrainer></span></span>|
|<span data-ttu-id="d8192-143">L-BFGS</span><span class="sxs-lookup"><span data-stu-id="d8192-143">L-BFGS</span></span>|<span data-ttu-id="d8192-144">기능 수가 클 경우에 사용합니다.</span><span class="sxs-lookup"><span data-stu-id="d8192-144">Use when number of features is large.</span></span> <span data-ttu-id="d8192-145">로지스틱 회귀 학습 통계를 생성하지만 AveragedPerceptronTrainer처럼 규모 조정은 안 됩니다.</span><span class="sxs-lookup"><span data-stu-id="d8192-145">Produces logistic regression training statistics, but doesn't scale as well as the AveragedPerceptronTrainer</span></span>|<span data-ttu-id="d8192-146"><xref:Microsoft.ML.Trainers.LbfgsLogisticRegressionBinaryTrainer> <xref:Microsoft.ML.Trainers.LbfgsMaximumEntropyMulticlassTrainer> <xref:Microsoft.ML.Trainers.LbfgsPoissonRegressionTrainer></span><span class="sxs-lookup"><span data-stu-id="d8192-146"><xref:Microsoft.ML.Trainers.LbfgsLogisticRegressionBinaryTrainer> <xref:Microsoft.ML.Trainers.LbfgsMaximumEntropyMulticlassTrainer> <xref:Microsoft.ML.Trainers.LbfgsPoissonRegressionTrainer></span></span>|
|<span data-ttu-id="d8192-147">기호 확률적 경사 하강법(Symbolic stochastic gradient descent)</span><span class="sxs-lookup"><span data-stu-id="d8192-147">Symbolic stochastic gradient descent</span></span>|<span data-ttu-id="d8192-148">가장 빠르고 가장 정확한 선형 이진 분류 트레이너입니다.</span><span class="sxs-lookup"><span data-stu-id="d8192-148">Fastest and most accurate linear binary classification trainer.</span></span> <span data-ttu-id="d8192-149">다수의 프로세서에 맞게 규모 조정이 가능합니다.</span><span class="sxs-lookup"><span data-stu-id="d8192-149">Scales well with number of processors</span></span>|<xref:Microsoft.ML.Trainers.SymbolicSgdLogisticRegressionBinaryTrainer>|

## <a name="decision-tree-algorithms"></a><span data-ttu-id="d8192-150">의사결정 트리 알고리즘</span><span class="sxs-lookup"><span data-stu-id="d8192-150">Decision tree algorithms</span></span>

<span data-ttu-id="d8192-151">의사결정 트리 알고리즘은 일련의 의사결정을 포함한 모델을 생성합니다(데이터 값을 통해 효과적으로 흐름 차트).</span><span class="sxs-lookup"><span data-stu-id="d8192-151">Decision tree algorithms create a model that contains a series of decisions: effectively a flow chart through the data values.</span></span>

<span data-ttu-id="d8192-152">이 유형의 알고리즘을 사용하기 위해 기능이 선형적으로 구분 가능해야 할 필요는 없습니다.</span><span class="sxs-lookup"><span data-stu-id="d8192-152">Features do not need to be linearly separable to use this type of algorithm.</span></span> <span data-ttu-id="d8192-153">그리고 기능 벡터의 개별 값이 의사결정 프로세스에서 독립적으로 사용되므로 기능을 일반화할 필요가 없습니다.</span><span class="sxs-lookup"><span data-stu-id="d8192-153">And features do not need to be normalized, because the individual values in the feature vector are used independently in the decision process.</span></span>

<span data-ttu-id="d8192-154">의사결정 트리 알고리즘은 일반적으로 매우 정확합니다.</span><span class="sxs-lookup"><span data-stu-id="d8192-154">Decision tree algorithms are generally very accurate.</span></span>

<span data-ttu-id="d8192-155">일반화 가법 모델(Generalized Additive Model, GAM)을 제외한 세 모델은 기능 수가 클 경우 설명 가능성이 부족할 수 있습니다.</span><span class="sxs-lookup"><span data-stu-id="d8192-155">Except for Generalized Additive Models (GAMs), tree models can lack explainability when the number of features is large.</span></span>

<span data-ttu-id="d8192-156">의사결정 트리 알고리즘은 더 많은 리소스를 사용하며 선형 알고리즘처럼 규모 조정은 안 됩니다.</span><span class="sxs-lookup"><span data-stu-id="d8192-156">Decision tree algorithms take more resources and do not scale as well as linear ones do.</span></span> <span data-ttu-id="d8192-157">이 알고리즘은 메모리에 적합할 수 있는 데이터 세트에서 잘 작용합니다.</span><span class="sxs-lookup"><span data-stu-id="d8192-157">They do perform well on datasets that can fit into memory.</span></span>

<span data-ttu-id="d8192-158">부스팅된 의사결정 트리는 작은 트리의 모음으로, 각 트리는 입력 데이터를 채점하고 더 나은 점수를 얻을 수 있도록 이 점수를 다음 트리에 계속 전달하며 모음의 각 트리는 이전 트리에서 개선됩니다.</span><span class="sxs-lookup"><span data-stu-id="d8192-158">Boosted decision trees are an ensemble of small trees where each tree scores the input data and passes the score onto the next tree to produce a better score, and so on, where each tree in the ensemble improves on the previous.</span></span>

<span data-ttu-id="d8192-159">**의사결정 트리 트레이너**</span><span class="sxs-lookup"><span data-stu-id="d8192-159">**Decision tree trainers**</span></span>

|<span data-ttu-id="d8192-160">알고리즘</span><span class="sxs-lookup"><span data-stu-id="d8192-160">Algorithm</span></span>|<span data-ttu-id="d8192-161">속성</span><span class="sxs-lookup"><span data-stu-id="d8192-161">Properties</span></span>|<span data-ttu-id="d8192-162">트레이너</span><span class="sxs-lookup"><span data-stu-id="d8192-162">Trainers</span></span>|
|---------|----------|--------|
|<span data-ttu-id="d8192-163">가벼운 경사 부스팅 머신(Light gradient boosted machine, LGBM)</span><span class="sxs-lookup"><span data-stu-id="d8192-163">Light gradient boosted machine</span></span>|<span data-ttu-id="d8192-164">이진 분류 트리 트레이너 중 가장 빠르고 정확하며</span><span class="sxs-lookup"><span data-stu-id="d8192-164">Fastest and most accurate of the binary classification tree trainers.</span></span> <span data-ttu-id="d8192-165">튜닝 성능이 뛰어납니다.</span><span class="sxs-lookup"><span data-stu-id="d8192-165">Highly tunable</span></span>|<span data-ttu-id="d8192-166"><xref:Microsoft.ML.Trainers.LightGbm.LightGbmBinaryTrainer> <xref:Microsoft.ML.Trainers.LightGbm.LightGbmMulticlassTrainer> <xref:Microsoft.ML.Trainers.LightGbm.LightGbmRegressionTrainer> <xref:Microsoft.ML.Trainers.LightGbm.LightGbmRankingTrainer></span><span class="sxs-lookup"><span data-stu-id="d8192-166"><xref:Microsoft.ML.Trainers.LightGbm.LightGbmBinaryTrainer> <xref:Microsoft.ML.Trainers.LightGbm.LightGbmMulticlassTrainer> <xref:Microsoft.ML.Trainers.LightGbm.LightGbmRegressionTrainer> <xref:Microsoft.ML.Trainers.LightGbm.LightGbmRankingTrainer></span></span>|
|<span data-ttu-id="d8192-167">빠른 트리(Fast tree)</span><span class="sxs-lookup"><span data-stu-id="d8192-167">Fast tree</span></span>|<span data-ttu-id="d8192-168">기능화된 이미지 데이터에 사용합니다.</span><span class="sxs-lookup"><span data-stu-id="d8192-168">Use for featurized image data.</span></span> <span data-ttu-id="d8192-169">불균형된 데이터에 유연하며</span><span class="sxs-lookup"><span data-stu-id="d8192-169">Resilient to unbalanced data.</span></span> <span data-ttu-id="d8192-170">튜닝 성능이 뛰어납니다.</span><span class="sxs-lookup"><span data-stu-id="d8192-170">Highly tunable</span></span> | <span data-ttu-id="d8192-171"><xref:Microsoft.ML.Trainers.FastTree.FastTreeBinaryTrainer> <xref:Microsoft.ML.Trainers.FastTree.FastTreeRegressionTrainer> <xref:Microsoft.ML.Trainers.FastTree.FastTreeTweedieTrainer> <xref:Microsoft.ML.Trainers.FastTree.FastTreeRankingTrainer></span><span class="sxs-lookup"><span data-stu-id="d8192-171"><xref:Microsoft.ML.Trainers.FastTree.FastTreeBinaryTrainer> <xref:Microsoft.ML.Trainers.FastTree.FastTreeRegressionTrainer> <xref:Microsoft.ML.Trainers.FastTree.FastTreeTweedieTrainer> <xref:Microsoft.ML.Trainers.FastTree.FastTreeRankingTrainer></span></span>|
|<span data-ttu-id="d8192-172">빠른 포레스트(Fast forest)</span><span class="sxs-lookup"><span data-stu-id="d8192-172">Fast forest</span></span>|<span data-ttu-id="d8192-173">노이즈가 많은 데이터와 잘 작동합니다.</span><span class="sxs-lookup"><span data-stu-id="d8192-173">Works well with noisy data</span></span>|<span data-ttu-id="d8192-174"><xref:Microsoft.ML.Trainers.FastTree.FastForestBinaryTrainer> <xref:Microsoft.ML.Trainers.FastTree.FastForestRegressionTrainer></span><span class="sxs-lookup"><span data-stu-id="d8192-174"><xref:Microsoft.ML.Trainers.FastTree.FastForestBinaryTrainer> <xref:Microsoft.ML.Trainers.FastTree.FastForestRegressionTrainer></span></span>|
|<span data-ttu-id="d8192-175">일반화된 가법 모델(Generalized additive model, GAM)</span><span class="sxs-lookup"><span data-stu-id="d8192-175">Generalized additive model (GAM)</span></span>|<span data-ttu-id="d8192-176">트리 알고리즘과 잘 작동하지만 설명 가능성이 우선인 문제에 최적입니다.</span><span class="sxs-lookup"><span data-stu-id="d8192-176">Best for problems that perform well with tree algorithms but where explainability is a priority</span></span>|<span data-ttu-id="d8192-177"><xref:Microsoft.ML.Trainers.FastTree.GamBinaryTrainer> <xref:Microsoft.ML.Trainers.FastTree.GamRegressionTrainer></span><span class="sxs-lookup"><span data-stu-id="d8192-177"><xref:Microsoft.ML.Trainers.FastTree.GamBinaryTrainer> <xref:Microsoft.ML.Trainers.FastTree.GamRegressionTrainer></span></span>|

## <a name="matrix-factorization"></a><span data-ttu-id="d8192-178">행렬 인수 분해(Matrix factorization）</span><span class="sxs-lookup"><span data-stu-id="d8192-178">Matrix factorization</span></span>

|<span data-ttu-id="d8192-179">속성</span><span class="sxs-lookup"><span data-stu-id="d8192-179">Properties</span></span>|<span data-ttu-id="d8192-180">트레이너</span><span class="sxs-lookup"><span data-stu-id="d8192-180">Trainers</span></span>|
|----------|--------|
|<span data-ttu-id="d8192-181">데이터 세트가 큰 분산된 범주 데이터에 최적입니다.</span><span class="sxs-lookup"><span data-stu-id="d8192-181">Best for sparse categorical data, with large datasets</span></span>|<xref:Microsoft.ML.Trainers.FieldAwareFactorizationMachineTrainer>|

## <a name="meta-algorithms"></a><span data-ttu-id="d8192-182">메타 알고리즘</span><span class="sxs-lookup"><span data-stu-id="d8192-182">Meta algorithms</span></span>

<span data-ttu-id="d8192-183">이진 트레이너에서 다중 클래스 트레이너를 만드는 트레이너입니다.</span><span class="sxs-lookup"><span data-stu-id="d8192-183">These trainers create a multi-class trainer from a binary trainer.</span></span> <span data-ttu-id="d8192-184"><xref:Microsoft.ML.Trainers.AveragedPerceptronTrainer>, <xref:Microsoft.ML.Trainers.LbfgsLogisticRegressionBinaryTrainer>, <xref:Microsoft.ML.Trainers.SymbolicSgdLogisticRegressionBinaryTrainer>, <xref:Microsoft.ML.Trainers.LightGbm.LightGbmBinaryTrainer>, <xref:Microsoft.ML.Trainers.FastTree.FastTreeBinaryTrainer>, <xref:Microsoft.ML.Trainers.FastTree.FastForestBinaryTrainer>, <xref:Microsoft.ML.Trainers.FastTree.GamBinaryTrainer>에 사용합니다.</span><span class="sxs-lookup"><span data-stu-id="d8192-184">Use with <xref:Microsoft.ML.Trainers.AveragedPerceptronTrainer>, <xref:Microsoft.ML.Trainers.LbfgsLogisticRegressionBinaryTrainer>, <xref:Microsoft.ML.Trainers.SymbolicSgdLogisticRegressionBinaryTrainer>, <xref:Microsoft.ML.Trainers.LightGbm.LightGbmBinaryTrainer>, <xref:Microsoft.ML.Trainers.FastTree.FastTreeBinaryTrainer>, <xref:Microsoft.ML.Trainers.FastTree.FastForestBinaryTrainer>, <xref:Microsoft.ML.Trainers.FastTree.GamBinaryTrainer>.</span></span>

|<span data-ttu-id="d8192-185">알고리즘</span><span class="sxs-lookup"><span data-stu-id="d8192-185">Algorithm</span></span>|<span data-ttu-id="d8192-186">속성</span><span class="sxs-lookup"><span data-stu-id="d8192-186">Properties</span></span>|<span data-ttu-id="d8192-187">트레이너</span><span class="sxs-lookup"><span data-stu-id="d8192-187">Trainers</span></span>|
|---------|----------|--------|
|<span data-ttu-id="d8192-188">One-Vs-One</span><span class="sxs-lookup"><span data-stu-id="d8192-188">One versus all</span></span>|<span data-ttu-id="d8192-189">이 다중 클래스 분류자는 각 클래스에 대해 하나의 이진 분류자를 학습하여 해당 클래스를 다른 모든 클래스와 구별합니다.</span><span class="sxs-lookup"><span data-stu-id="d8192-189">This multiclass classifier trains one binary classifier for each class, which distinguishes that class from all other classes.</span></span> <span data-ttu-id="d8192-190">범주화할 클래스 수에 따라 규모 조정이 제한적입니다.</span><span class="sxs-lookup"><span data-stu-id="d8192-190">Is limited in scale by the number of classes to categorize</span></span>|[<span data-ttu-id="d8192-191">OneVersusAllTrainer\<BinaryClassificationTrainer></span><span class="sxs-lookup"><span data-stu-id="d8192-191">OneVersusAllTrainer\<BinaryClassificationTrainer></span></span>](xref:Microsoft.ML.Trainers.OneVersusAllTrainer) |
|<span data-ttu-id="d8192-192">쌍별 결합(Pairwise coupling)</span><span class="sxs-lookup"><span data-stu-id="d8192-192">Pairwise coupling</span></span>|<span data-ttu-id="d8192-193">이 다중 클래스 분류자는 각 쌍의 클래스에서 이진 분류 알고리즘을 학습합니다.</span><span class="sxs-lookup"><span data-stu-id="d8192-193">This multiclass classifier trains a binary classification algorithm on each pair of classes.</span></span> <span data-ttu-id="d8192-194">두 클래스의 각 조합을 학습해야 하므로 클래스 수에 따라 규모 조정이 제한적입니다.</span><span class="sxs-lookup"><span data-stu-id="d8192-194">Is limited in scale by the number of classes, as each combination of two classes must be trained.</span></span>|[<span data-ttu-id="d8192-195">PairwiseCouplingTrainer\<BinaryClassificationTrainer></span><span class="sxs-lookup"><span data-stu-id="d8192-195">PairwiseCouplingTrainer\<BinaryClassificationTrainer></span></span>](xref:Microsoft.ML.Trainers.PairwiseCouplingTrainer)|

## <a name="k-means"></a><span data-ttu-id="d8192-196">K-평균(K-means)</span><span class="sxs-lookup"><span data-stu-id="d8192-196">K-Means</span></span>

|<span data-ttu-id="d8192-197">속성</span><span class="sxs-lookup"><span data-stu-id="d8192-197">Properties</span></span>|<span data-ttu-id="d8192-198">트레이너</span><span class="sxs-lookup"><span data-stu-id="d8192-198">Trainers</span></span>|
|----------|--------|
|<span data-ttu-id="d8192-199">클러스터링에 사용</span><span class="sxs-lookup"><span data-stu-id="d8192-199">Use for clustering</span></span>|<xref:Microsoft.ML.Trainers.KMeansTrainer>|

## <a name="principal-component-analysis"></a><span data-ttu-id="d8192-200">주성분 분석(Principal component analysis)</span><span class="sxs-lookup"><span data-stu-id="d8192-200">Principal component analysis</span></span>

|<span data-ttu-id="d8192-201">속성</span><span class="sxs-lookup"><span data-stu-id="d8192-201">Properties</span></span>|<span data-ttu-id="d8192-202">트레이너</span><span class="sxs-lookup"><span data-stu-id="d8192-202">Trainers</span></span>|
|----------|--------|
|<span data-ttu-id="d8192-203">변칙 검색에 사용</span><span class="sxs-lookup"><span data-stu-id="d8192-203">Use for anomaly detection</span></span>|<xref:Microsoft.ML.Trainers.RandomizedPcaTrainer>|

## <a name="naive-bayes"></a><span data-ttu-id="d8192-204">Naive Bayes</span><span class="sxs-lookup"><span data-stu-id="d8192-204">Naive Bayes</span></span>

|<span data-ttu-id="d8192-205">속성</span><span class="sxs-lookup"><span data-stu-id="d8192-205">Properties</span></span>|<span data-ttu-id="d8192-206">트레이너</span><span class="sxs-lookup"><span data-stu-id="d8192-206">Trainers</span></span>|
|----------|--------|
|<span data-ttu-id="d8192-207">기능이 독립적이고 학습 데이터 세트가 작을 때 이 다중 클래스 분류 트레이너를 사용합니다.</span><span class="sxs-lookup"><span data-stu-id="d8192-207">Use this multi-class classification trainer when the features are independent, and the training dataset is small.</span></span>|<xref:Microsoft.ML.Trainers.NaiveBayesMulticlassTrainer>|

## <a name="prior-trainer"></a><span data-ttu-id="d8192-208">사전 트레이너(Prior Trainer）</span><span class="sxs-lookup"><span data-stu-id="d8192-208">Prior Trainer</span></span>

|<span data-ttu-id="d8192-209">속성</span><span class="sxs-lookup"><span data-stu-id="d8192-209">Properties</span></span>|<span data-ttu-id="d8192-210">트레이너</span><span class="sxs-lookup"><span data-stu-id="d8192-210">Trainers</span></span>|
|----------|--------|
|<span data-ttu-id="d8192-211">다른 트레이너의 성과를 기준치로 삼기 위해 이 이진 분류 트레이너를 사용합니다.</span><span class="sxs-lookup"><span data-stu-id="d8192-211">Use this binary classification trainer to baseline the performance of other trainers.</span></span> <span data-ttu-id="d8192-212">효과를 발휘하려면 다른 트레이너의 메트릭이 사전 트레이너보다 더 좋아야 합니다.</span><span class="sxs-lookup"><span data-stu-id="d8192-212">To be effective, the metrics of the other trainers should be better than the prior trainer.</span></span> |<xref:Microsoft.ML.Trainers.PriorTrainer>|
